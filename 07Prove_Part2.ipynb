{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07Prove:Part2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPiDH1K3P8znYFHw3fDyazu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshEthan/Prove06/blob/master/07Prove_Part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y7atHXjpxQW",
        "colab_type": "text"
      },
      "source": [
        "# Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gPqn2Z7k5ap",
        "colab_type": "text"
      },
      "source": [
        "* Use a library implementation to make predictions on several different datasets\n",
        "* Make predictions on at least **four** different datasets.\n",
        "* Among your datasets make sure that you worked with the following characteristics:\n",
        "    * Text values\n",
        "    * Binary classification (only two possible values)\n",
        "    * Classification with more than two target values\n",
        "    * Regression (numeric target values)\n",
        "    * Look at different measures of success for your datasets (more than just accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjExW-uAp1SD",
        "colab_type": "text"
      },
      "source": [
        "# Data Set 1 - League of Legends"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQo3y4qLYEQB",
        "colab_type": "code",
        "outputId": "f61929e1-2411-4330-e672-954cc9bf6fb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LOAD DATA\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 500)\n",
        "data = pd.read_csv(\"/content/high_diamond_ranked_10min.csv\")\n",
        "\n",
        "\n",
        "#SPLIT INTO X AND Y\n",
        "X = data.drop(columns=[\"gameId\", \"blueWins\"]).astype('int32').to_numpy()\n",
        "y = data.blueWins.to_numpy()\n",
        "\n",
        "\n",
        "#NORMALIZE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "#TRAINING AND TESTING SET\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1)\n",
        "\n",
        "\n",
        "#### BUILDING THE MODEL ###\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model = Sequential()    # Specifies we are creating model sequentially \n",
        "                        #   and the each output layer is input to the next layer.\n",
        "\n",
        "# ADD EACH LAYER\n",
        "model.add(Dense(20, input_dim=38, activation='relu'))    # 'model.add' add a layer to our neural network.\n",
        "                                                        #   'Dense' specify fully connected layer.\n",
        "                                                        #       Arguments: Output(16), Input(20), Activation Function(relu)\n",
        "model.add(Dense(1, activation='sigmoid')) # X layer we dont need to specify input(16), the model to be sequential\n",
        "\n",
        "\n",
        "# LOSS FUNCTION/OPTIMIZER\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])   # 'Categorical_crossentropy' multiple classes.\n",
        "                                                                                    # Metrics is used to specify \n",
        "                                                                                    #   the way to judge the performance.\n",
        "                                                                                    # 'loss' what to use to evaluate a set of weights\n",
        "                                                                                    # 'optimizer' is used to search through different weights\n",
        "                                                                                    # “adam“ stochastic gradient descent algorithm \n",
        "                                                                                    # 'accuracy' because it is a classification problem\n",
        "\n",
        "\n",
        "\n",
        "# LOSS FUNCTION/OPTIMIZER\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])   # 'Categorical_crossentropy' multiple classes.\n",
        "                                                                                    # Metrics is used to specify \n",
        "                                                                                    #   the way to judge the performance.\n",
        "                                                                                    # 'loss' what to use to evaluate a set of weights\n",
        "                                                                                    # 'optimizer' is used to search through different weights\n",
        "                                                                                    # “adam“ stochastic gradient descent algorithm \n",
        "                                                                                    # 'accuracy' because it is a classification problem\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#TRAINING\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=100)    # Specify: \n",
        "                                                                    #   input data-> X_train, \n",
        "                                                                    #   labels-> y_train, \n",
        "                                                                    #   number of epochs(iterations), \n",
        "                                                                    #   and batch size. \n",
        "                                                                    # Returns history of model training.\n",
        "                                                                    # Batch Size: \n",
        "                                                                    #   Divides data into batches = batch_size. \n",
        "                                                                    #   Only this number of samples will be loaded/processed. \n",
        "                                                                    #   Once we are done with one batch, next batch will be processed.\n",
        "                                                                    # Epoch: One pass through all of the rows in the training dataset.\n",
        "                                                                    #   One epoch is comprised of one or more batches\n",
        "\n",
        "\n",
        "#Use your network to make predictions about a test set and note the success of the algorithm on your dataset.\n",
        "\n",
        "#Experiment with at least 3 different sets of hyper-parameters for the algorithm \n",
        "#   (e.g., different number of layers and nodes per layer, different learning rates, different activation functions, etc.).\n",
        "\n",
        "# check the model’s performance\n",
        "_, accuracy = model.evaluate(X, y, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8891/8891 [==============================] - 0s 21us/step - loss: 0.6561 - accuracy: 0.6357\n",
            "Epoch 2/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5499 - accuracy: 0.7156\n",
            "Epoch 3/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5407 - accuracy: 0.7204\n",
            "Epoch 4/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5358 - accuracy: 0.7224\n",
            "Epoch 5/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5326 - accuracy: 0.7267\n",
            "Epoch 6/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5307 - accuracy: 0.7268\n",
            "Epoch 7/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5290 - accuracy: 0.7296\n",
            "Epoch 8/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5275 - accuracy: 0.7301\n",
            "Epoch 9/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5267 - accuracy: 0.7306\n",
            "Epoch 10/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5256 - accuracy: 0.7294\n",
            "Epoch 11/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5248 - accuracy: 0.7309\n",
            "Epoch 12/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5240 - accuracy: 0.7310\n",
            "Epoch 13/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5232 - accuracy: 0.7313\n",
            "Epoch 14/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5229 - accuracy: 0.7329\n",
            "Epoch 15/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5222 - accuracy: 0.7341\n",
            "Epoch 16/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5215 - accuracy: 0.7339\n",
            "Epoch 17/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5210 - accuracy: 0.7357\n",
            "Epoch 18/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5209 - accuracy: 0.7351\n",
            "Epoch 19/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5205 - accuracy: 0.7346\n",
            "Epoch 20/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5198 - accuracy: 0.7356\n",
            "Epoch 21/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5192 - accuracy: 0.7345\n",
            "Epoch 22/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5189 - accuracy: 0.7355\n",
            "Epoch 23/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5184 - accuracy: 0.7358\n",
            "Epoch 24/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5182 - accuracy: 0.7380\n",
            "Epoch 25/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5178 - accuracy: 0.7379\n",
            "Epoch 26/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5176 - accuracy: 0.7379\n",
            "Epoch 27/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5174 - accuracy: 0.7373\n",
            "Epoch 28/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5171 - accuracy: 0.7389\n",
            "Epoch 29/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5167 - accuracy: 0.7393\n",
            "Epoch 30/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5165 - accuracy: 0.7388\n",
            "Epoch 31/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5163 - accuracy: 0.7393\n",
            "Epoch 32/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5161 - accuracy: 0.7384\n",
            "Epoch 33/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5161 - accuracy: 0.7391\n",
            "Epoch 34/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5154 - accuracy: 0.7403\n",
            "Epoch 35/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5155 - accuracy: 0.7394\n",
            "Epoch 36/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5151 - accuracy: 0.7398\n",
            "Epoch 37/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5150 - accuracy: 0.7397\n",
            "Epoch 38/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5145 - accuracy: 0.7394\n",
            "Epoch 39/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5144 - accuracy: 0.7398\n",
            "Epoch 40/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5143 - accuracy: 0.7413\n",
            "Epoch 41/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5139 - accuracy: 0.7411\n",
            "Epoch 42/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5139 - accuracy: 0.7396\n",
            "Epoch 43/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5137 - accuracy: 0.7410\n",
            "Epoch 44/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5138 - accuracy: 0.7419\n",
            "Epoch 45/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5133 - accuracy: 0.7400\n",
            "Epoch 46/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5131 - accuracy: 0.7409\n",
            "Epoch 47/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5126 - accuracy: 0.7413\n",
            "Epoch 48/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5126 - accuracy: 0.7410\n",
            "Epoch 49/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5127 - accuracy: 0.7398\n",
            "Epoch 50/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5122 - accuracy: 0.7404\n",
            "Epoch 51/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5119 - accuracy: 0.7415\n",
            "Epoch 52/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5117 - accuracy: 0.7403\n",
            "Epoch 53/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5113 - accuracy: 0.7402\n",
            "Epoch 54/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5115 - accuracy: 0.7407\n",
            "Epoch 55/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5109 - accuracy: 0.7410\n",
            "Epoch 56/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5108 - accuracy: 0.7398\n",
            "Epoch 57/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5103 - accuracy: 0.7419\n",
            "Epoch 58/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5103 - accuracy: 0.7421\n",
            "Epoch 59/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5101 - accuracy: 0.7437\n",
            "Epoch 60/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5101 - accuracy: 0.7423\n",
            "Epoch 61/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5098 - accuracy: 0.7420\n",
            "Epoch 62/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5093 - accuracy: 0.7419\n",
            "Epoch 63/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5094 - accuracy: 0.7430\n",
            "Epoch 64/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5094 - accuracy: 0.7425\n",
            "Epoch 65/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5087 - accuracy: 0.7428\n",
            "Epoch 66/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5085 - accuracy: 0.7414\n",
            "Epoch 67/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5087 - accuracy: 0.7438\n",
            "Epoch 68/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5084 - accuracy: 0.7443\n",
            "Epoch 69/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5079 - accuracy: 0.7436\n",
            "Epoch 70/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5079 - accuracy: 0.7439\n",
            "Epoch 71/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5079 - accuracy: 0.7432\n",
            "Epoch 72/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5077 - accuracy: 0.7448\n",
            "Epoch 73/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5079 - accuracy: 0.7443\n",
            "Epoch 74/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5072 - accuracy: 0.7443\n",
            "Epoch 75/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5072 - accuracy: 0.7452\n",
            "Epoch 76/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5073 - accuracy: 0.7441\n",
            "Epoch 77/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5069 - accuracy: 0.7438\n",
            "Epoch 78/100\n",
            "8891/8891 [==============================] - 0s 12us/step - loss: 0.5069 - accuracy: 0.7447\n",
            "Epoch 79/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5069 - accuracy: 0.7445\n",
            "Epoch 80/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5062 - accuracy: 0.7436\n",
            "Epoch 81/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5062 - accuracy: 0.7454\n",
            "Epoch 82/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5063 - accuracy: 0.7461\n",
            "Epoch 83/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5061 - accuracy: 0.7448\n",
            "Epoch 84/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5060 - accuracy: 0.7455\n",
            "Epoch 85/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5059 - accuracy: 0.7428\n",
            "Epoch 86/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5055 - accuracy: 0.7459\n",
            "Epoch 87/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5051 - accuracy: 0.7454\n",
            "Epoch 88/100\n",
            "8891/8891 [==============================] - 0s 15us/step - loss: 0.5054 - accuracy: 0.7465\n",
            "Epoch 89/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5049 - accuracy: 0.7464\n",
            "Epoch 90/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5053 - accuracy: 0.7466\n",
            "Epoch 91/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5049 - accuracy: 0.7476\n",
            "Epoch 92/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5045 - accuracy: 0.7452\n",
            "Epoch 93/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5047 - accuracy: 0.7475\n",
            "Epoch 94/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5040 - accuracy: 0.7456\n",
            "Epoch 95/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5042 - accuracy: 0.7482\n",
            "Epoch 96/100\n",
            "8891/8891 [==============================] - 0s 14us/step - loss: 0.5036 - accuracy: 0.7469\n",
            "Epoch 97/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5040 - accuracy: 0.7474\n",
            "Epoch 98/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5045 - accuracy: 0.7476\n",
            "Epoch 99/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5035 - accuracy: 0.7477\n",
            "Epoch 100/100\n",
            "8891/8891 [==============================] - 0s 13us/step - loss: 0.5034 - accuracy: 0.7472\n",
            "Accuracy: 74.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E19Hd3o2sD1Q"
      },
      "source": [
        "# Data Set 2 - Spotify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "33e930f6-e6bf-4347-b6d8-efd1508c0d84",
        "id": "Z1MHPLEpsD1Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LOAD DATA\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 500)\n",
        "data = pd.read_csv(\"/content/data.csv\")\n",
        "\n",
        "#SPLIT INTO X AND Y\n",
        "X = data.drop(columns=[\"artists\", \"id\", \"name\", \"release_date\", \"popularity\"]).astype('int32').to_numpy()\n",
        "y = data.popularity.to_numpy()\n",
        "\n",
        "#NORMALIZE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "#TRAINING AND TESTING SET\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .1)\n",
        "\n",
        "#### BUILDING THE MODEL ###\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model = Sequential()    # Specifies we are creating model sequentially \n",
        "                        #   and the each output layer is input to the next layer.\n",
        "\n",
        "# ADD EACH LAYER\n",
        "model.add(Dense(100, input_dim=15, activation='relu'))    # 'model.add' add a layer to our neural network.\n",
        "                                                        #   'Dense' specify fully connected layer.\n",
        "                                                        #       Arguments: Output(16), Input(20), Activation Function(relu)\n",
        "model.add(Dense(1, activation='sigmoid')) # X layer we dont need to specify input(16), the model to be sequential\n",
        "\n",
        "\n",
        "# LOSS FUNCTION/OPTIMIZER\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])   # 'Categorical_crossentropy' multiple classes.\n",
        "                                                                                    # Metrics is used to specify \n",
        "                                                                                    #   the way to judge the performance.\n",
        "                                                                                    # 'loss' what to use to evaluate a set of weights\n",
        "                                                                                    # 'optimizer' is used to search through different weights\n",
        "                                                                                    # “adam“ stochastic gradient descent algorithm \n",
        "                                                                                    # 'accuracy' because it is a classification problem\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#TRAINING\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10000)    # Specify: \n",
        "                                                                    #   input data-> X_train, \n",
        "                                                                    #   labels-> y_train, \n",
        "                                                                    #   number of epochs(iterations), \n",
        "                                                                    #   and batch size. \n",
        "                                                                    # Returns history of model training.\n",
        "                                                                    # Batch Size: \n",
        "                                                                    #   Divides data into batches = batch_size. \n",
        "                                                                    #   Only this number of samples will be loaded/processed. \n",
        "                                                                    #   Once we are done with one batch, next batch will be processed.\n",
        "                                                                    # Epoch: One pass through all of the rows in the training dataset.\n",
        "                                                                    #   One epoch is comprised of one or more batches\n",
        "\n",
        "\n",
        "#Use your network to make predictions about a test set and note the success of the algorithm on your dataset.\n",
        "\n",
        "#Experiment with at least 3 different sets of hyper-parameters for the algorithm \n",
        "#   (e.g., different number of layers and nodes per layer, different learning rates, different activation functions, etc.).\n",
        "\n",
        "# check the model’s performance\n",
        "_, accuracy = model.evaluate(X, y, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1423.7063 - accuracy: 0.0688\n",
            "Epoch 2/100\n",
            "151732/151732 [==============================] - 0s 1us/step - loss: 1415.4594 - accuracy: 0.0289\n",
            "Epoch 3/100\n",
            "151732/151732 [==============================] - 0s 1us/step - loss: 1409.2335 - accuracy: 0.0144\n",
            "Epoch 4/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1405.1199 - accuracy: 0.0142\n",
            "Epoch 5/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1402.4752 - accuracy: 0.0142\n",
            "Epoch 6/100\n",
            "151732/151732 [==============================] - 0s 1us/step - loss: 1400.7544 - accuracy: 0.0142\n",
            "Epoch 7/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1399.6191 - accuracy: 0.0142\n",
            "Epoch 8/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1398.8478 - accuracy: 0.0142\n",
            "Epoch 9/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1398.3115 - accuracy: 0.0142\n",
            "Epoch 10/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1397.9279 - accuracy: 0.0142\n",
            "Epoch 11/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1397.6456 - accuracy: 0.0142\n",
            "Epoch 12/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1397.4324 - accuracy: 0.0142\n",
            "Epoch 13/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1397.2680 - accuracy: 0.0142\n",
            "Epoch 14/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1397.1384 - accuracy: 0.0142\n",
            "Epoch 15/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1397.0346 - accuracy: 0.0142\n",
            "Epoch 16/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.9505 - accuracy: 0.0142\n",
            "Epoch 17/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.8811 - accuracy: 0.0142\n",
            "Epoch 18/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.8233 - accuracy: 0.0142\n",
            "Epoch 19/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.7748 - accuracy: 0.0142\n",
            "Epoch 20/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.7335 - accuracy: 0.0142\n",
            "Epoch 21/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.6981 - accuracy: 0.0142\n",
            "Epoch 22/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.6676 - accuracy: 0.0142\n",
            "Epoch 23/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.6410 - accuracy: 0.0142\n",
            "Epoch 24/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.6179 - accuracy: 0.0142\n",
            "Epoch 25/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.5975 - accuracy: 0.0142\n",
            "Epoch 26/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.5795 - accuracy: 0.0142\n",
            "Epoch 27/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.5634 - accuracy: 0.0142\n",
            "Epoch 28/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.5491 - accuracy: 0.0142\n",
            "Epoch 29/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.5363 - accuracy: 0.0142\n",
            "Epoch 30/100\n",
            "151732/151732 [==============================] - 0s 1us/step - loss: 1396.5247 - accuracy: 0.0142\n",
            "Epoch 31/100\n",
            "151732/151732 [==============================] - 0s 1us/step - loss: 1396.5143 - accuracy: 0.0142\n",
            "Epoch 32/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.5048 - accuracy: 0.0142\n",
            "Epoch 33/100\n",
            "151732/151732 [==============================] - 0s 1us/step - loss: 1396.4962 - accuracy: 0.0142\n",
            "Epoch 34/100\n",
            "151732/151732 [==============================] - 0s 1us/step - loss: 1396.4884 - accuracy: 0.0142\n",
            "Epoch 35/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4812 - accuracy: 0.0142\n",
            "Epoch 36/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4746 - accuracy: 0.0142\n",
            "Epoch 37/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4686 - accuracy: 0.0142\n",
            "Epoch 38/100\n",
            "151732/151732 [==============================] - 0s 1us/step - loss: 1396.4630 - accuracy: 0.0142\n",
            "Epoch 39/100\n",
            "151732/151732 [==============================] - 0s 1us/step - loss: 1396.4578 - accuracy: 0.0142\n",
            "Epoch 40/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4530 - accuracy: 0.0142\n",
            "Epoch 41/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4486 - accuracy: 0.0142\n",
            "Epoch 42/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4446 - accuracy: 0.0142\n",
            "Epoch 43/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4406 - accuracy: 0.0142\n",
            "Epoch 44/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4371 - accuracy: 0.0142\n",
            "Epoch 45/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4337 - accuracy: 0.0142\n",
            "Epoch 46/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4306 - accuracy: 0.0142\n",
            "Epoch 47/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4276 - accuracy: 0.0142\n",
            "Epoch 48/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4249 - accuracy: 0.0142\n",
            "Epoch 49/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4223 - accuracy: 0.0142\n",
            "Epoch 50/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4198 - accuracy: 0.0142\n",
            "Epoch 51/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4175 - accuracy: 0.0142\n",
            "Epoch 52/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4154 - accuracy: 0.0142\n",
            "Epoch 53/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4133 - accuracy: 0.0142\n",
            "Epoch 54/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4114 - accuracy: 0.0142\n",
            "Epoch 55/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4095 - accuracy: 0.0142\n",
            "Epoch 56/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4078 - accuracy: 0.0142\n",
            "Epoch 57/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4062 - accuracy: 0.0142\n",
            "Epoch 58/100\n",
            "151732/151732 [==============================] - 0s 1us/step - loss: 1396.4046 - accuracy: 0.0142\n",
            "Epoch 59/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4031 - accuracy: 0.0142\n",
            "Epoch 60/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4017 - accuracy: 0.0142\n",
            "Epoch 61/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.4003 - accuracy: 0.0142\n",
            "Epoch 62/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3990 - accuracy: 0.0142\n",
            "Epoch 63/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3978 - accuracy: 0.0142\n",
            "Epoch 64/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3965 - accuracy: 0.0142\n",
            "Epoch 65/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3955 - accuracy: 0.0142\n",
            "Epoch 66/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3944 - accuracy: 0.0142\n",
            "Epoch 67/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3933 - accuracy: 0.0142\n",
            "Epoch 68/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3924 - accuracy: 0.0142\n",
            "Epoch 69/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3915 - accuracy: 0.0142\n",
            "Epoch 70/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3906 - accuracy: 0.0142\n",
            "Epoch 71/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3897 - accuracy: 0.0142\n",
            "Epoch 72/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3889 - accuracy: 0.0142\n",
            "Epoch 73/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3881 - accuracy: 0.0142\n",
            "Epoch 74/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3874 - accuracy: 0.0142\n",
            "Epoch 75/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3866 - accuracy: 0.0142\n",
            "Epoch 76/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3859 - accuracy: 0.0142\n",
            "Epoch 77/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3852 - accuracy: 0.0142\n",
            "Epoch 78/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3845 - accuracy: 0.0142\n",
            "Epoch 79/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3840 - accuracy: 0.0142\n",
            "Epoch 80/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3833 - accuracy: 0.0142\n",
            "Epoch 81/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3828 - accuracy: 0.0142\n",
            "Epoch 82/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3822 - accuracy: 0.0142\n",
            "Epoch 83/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3817 - accuracy: 0.0142\n",
            "Epoch 84/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3812 - accuracy: 0.0142\n",
            "Epoch 85/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3806 - accuracy: 0.0142\n",
            "Epoch 86/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3802 - accuracy: 0.0142\n",
            "Epoch 87/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3797 - accuracy: 0.0142\n",
            "Epoch 88/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3793 - accuracy: 0.0142\n",
            "Epoch 89/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3788 - accuracy: 0.0142\n",
            "Epoch 90/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3784 - accuracy: 0.0142\n",
            "Epoch 91/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3781 - accuracy: 0.0142\n",
            "Epoch 92/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3776 - accuracy: 0.0142\n",
            "Epoch 93/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3773 - accuracy: 0.0142\n",
            "Epoch 94/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3769 - accuracy: 0.0142\n",
            "Epoch 95/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3766 - accuracy: 0.0142\n",
            "Epoch 96/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3762 - accuracy: 0.0142\n",
            "Epoch 97/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3759 - accuracy: 0.0142\n",
            "Epoch 98/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3756 - accuracy: 0.0142\n",
            "Epoch 99/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3753 - accuracy: 0.0142\n",
            "Epoch 100/100\n",
            "151732/151732 [==============================] - 0s 2us/step - loss: 1396.3750 - accuracy: 0.0142\n",
            "Accuracy: 1.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a6MhLeq4ft5D"
      },
      "source": [
        "# Data Set 3 - Movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "132968d8-0ec0-427b-83db-cdf01f62ffd2",
        "id": "NFqlg75Wft5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LOAD DATA\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 500)\n",
        "data = pd.read_csv(\"/content/MoviesOnStreamingPlatforms_updated.csv\")\n",
        "data.Age = data.Age.astype('category')\n",
        "data.Age = data.Age.cat.codes\n",
        "data[\"Rotten Tomatoes\"] = data[\"Rotten Tomatoes\"].astype('category')\n",
        "data[\"Rotten Tomatoes\"] = data[\"Rotten Tomatoes\"].cat.codes\n",
        "data.Language = data.Language.astype('category')\n",
        "data.Language = data.Language.cat.codes\n",
        "\n",
        "from numpy import nan\n",
        "data.dropna(inplace=True)\n",
        "print(data)\n",
        "\n",
        "\n",
        "#SPLIT INTO X AND \n",
        "X = data.drop(columns=[\"Title\", \"ID\", \"IMDb\", \"Directors\", \"Genres\", \"Country\"]).astype('int32').to_numpy()\n",
        "y = data.IMDb.to_numpy()\n",
        "\n",
        "#NORMALIZE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "#TRAINING AND TESTING SET\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .1)\n",
        "\n",
        "#### BUILDING THE MODEL ###\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model = Sequential()    # Specifies we are creating model sequentially \n",
        "                        #   and the each output layer is input to the next layer.\n",
        "\n",
        "# ADD EACH LAYER\n",
        "model.add(Dense(11, input_dim=11, activation='relu'))    # 'model.add' add a layer to our neural network.\n",
        "                                                        #   'Dense' specify fully connected layer.\n",
        "                                                        #       Arguments: Output(16), Input(20), Activation Function(relu)\n",
        "model.add(Dense(1, activation='sigmoid')) # X layer we dont need to specify input(16), the model to be sequential\n",
        "\n",
        "\n",
        "# LOSS FUNCTION/OPTIMIZER\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])   # 'Categorical_crossentropy' multiple classes.\n",
        "                                                                                    # Metrics is used to specify \n",
        "                                                                                    #   the way to judge the performance.\n",
        "                                                                                    # 'loss' what to use to evaluate a set of weights\n",
        "                                                                                    # 'optimizer' is used to search through different weights\n",
        "                                                                                    # “adam“ stochastic gradient descent algorithm \n",
        "                                                                                    # 'accuracy' because it is a classification problem\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#TRAINING\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10000)    # Specify: \n",
        "                                                                    #   input data-> X_train, \n",
        "                                                                    #   labels-> y_train, \n",
        "                                                                    #   number of epochs(iterations), \n",
        "                                                                    #   and batch size. \n",
        "                                                                    # Returns history of model training.\n",
        "                                                                    # Batch Size: \n",
        "                                                                    #   Divides data into batches = batch_size. \n",
        "                                                                    #   Only this number of samples will be loaded/processed. \n",
        "                                                                    #   Once we are done with one batch, next batch will be processed.\n",
        "                                                                    # Epoch: One pass through all of the rows in the training dataset.\n",
        "                                                                    #   One epoch is comprised of one or more batches\n",
        "\n",
        "\n",
        "#Use your network to make predictions about a test set and note the success of the algorithm on your dataset.\n",
        "\n",
        "#Experiment with at least 3 different sets of hyper-parameters for the algorithm \n",
        "#   (e.g., different number of layers and nodes per layer, different learning rates, different activation functions, etc.).\n",
        "\n",
        "# check the model’s performance\n",
        "_, accuracy = model.evaluate(X, y, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Unnamed: 0     ID                           Title  Year  Age  IMDb  \\\n",
            "0               0      1                       Inception  2010    0   8.8   \n",
            "1               1      2                      The Matrix  1999    2   8.7   \n",
            "2               2      3          Avengers: Infinity War  2018    0   8.5   \n",
            "3               3      4              Back to the Future  1985    3   8.5   \n",
            "4               4      5  The Good, the Bad and the Ugly  1966    2   8.8   \n",
            "...           ...    ...                             ...   ...  ...   ...   \n",
            "16734       16734  16735        Sultan And The Rock Star  1980   -1   5.9   \n",
            "16737       16737  16738                 The Bears and I  1974    4   6.2   \n",
            "16738       16738  16739    Whispers: An Elephant's Tale  2000    4   5.0   \n",
            "16739       16739  16740       The Ghosts of Buxley Hall  1980   -1   6.2   \n",
            "16740       16740  16741                  The Poof Point  2001    3   4.7   \n",
            "\n",
            "       Rotten Tomatoes  Netflix  Hulu  Prime Video  Disney+  Type  \\\n",
            "0                   85        1     0            0        0     0   \n",
            "1                   85        1     0            0        0     0   \n",
            "2                   82        1     0            0        0     0   \n",
            "3                   95        1     0            0        0     0   \n",
            "4                   96        1     0            1        0     0   \n",
            "...                ...      ...   ...          ...      ...   ...   \n",
            "16734               -1        0     0            0        1     0   \n",
            "16737               -1        0     0            0        1     0   \n",
            "16738               -1        0     0            0        1     0   \n",
            "16739               -1        0     0            0        1     0   \n",
            "16740               -1        0     0            0        1     0   \n",
            "\n",
            "                            Directors                            Genres  \\\n",
            "0                   Christopher Nolan  Action,Adventure,Sci-Fi,Thriller   \n",
            "1      Lana Wachowski,Lilly Wachowski                     Action,Sci-Fi   \n",
            "2             Anthony Russo,Joe Russo           Action,Adventure,Sci-Fi   \n",
            "3                     Robert Zemeckis           Adventure,Comedy,Sci-Fi   \n",
            "4                        Sergio Leone                           Western   \n",
            "...                               ...                               ...   \n",
            "16734                Edward M. Abroms            Adventure,Drama,Family   \n",
            "16737                Bernard McEveety                      Drama,Family   \n",
            "16738                  Dereck Joubert                  Adventure,Family   \n",
            "16739                    Bruce Bilson      Comedy,Family,Fantasy,Horror   \n",
            "16740                     Neal Israel              Comedy,Family,Sci-Fi   \n",
            "\n",
            "                            Country  Language  Runtime  \n",
            "0      United States,United Kingdom       350    148.0  \n",
            "1                     United States       101    136.0  \n",
            "2                     United States       101    149.0  \n",
            "3                     United States       101    116.0  \n",
            "4          Italy,Spain,West Germany       783    161.0  \n",
            "...                             ...       ...      ...  \n",
            "16734                 United States       101     60.0  \n",
            "16737                 United States       101     89.0  \n",
            "16738                 United States       101     72.0  \n",
            "16739                 United States       101    120.0  \n",
            "16740                 United States       101     90.0  \n",
            "\n",
            "[15388 rows x 17 columns]\n",
            "Epoch 1/100\n",
            "13849/13849 [==============================] - 0s 5us/step - loss: 30.0563 - accuracy: 1.4441e-04\n",
            "Epoch 2/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 30.0110 - accuracy: 1.4441e-04\n",
            "Epoch 3/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.9616 - accuracy: 1.4441e-04\n",
            "Epoch 4/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.9141 - accuracy: 1.4441e-04\n",
            "Epoch 5/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.8655 - accuracy: 1.4441e-04\n",
            "Epoch 6/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.8169 - accuracy: 1.4441e-04\n",
            "Epoch 7/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.7688 - accuracy: 1.4441e-04\n",
            "Epoch 8/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.7207 - accuracy: 1.4441e-04\n",
            "Epoch 9/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.6730 - accuracy: 1.4441e-04\n",
            "Epoch 10/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.6257 - accuracy: 1.4441e-04\n",
            "Epoch 11/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.5787 - accuracy: 1.4441e-04\n",
            "Epoch 12/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.5320 - accuracy: 1.4441e-04\n",
            "Epoch 13/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.4855 - accuracy: 1.4441e-04\n",
            "Epoch 14/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.4399 - accuracy: 1.4441e-04\n",
            "Epoch 15/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.3945 - accuracy: 1.4441e-04\n",
            "Epoch 16/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.3496 - accuracy: 1.4441e-04\n",
            "Epoch 17/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.3051 - accuracy: 1.4441e-04\n",
            "Epoch 18/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.2609 - accuracy: 1.4441e-04\n",
            "Epoch 19/100\n",
            "13849/13849 [==============================] - 0s 2us/step - loss: 29.2174 - accuracy: 1.4441e-04\n",
            "Epoch 20/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.1739 - accuracy: 1.4441e-04\n",
            "Epoch 21/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.1307 - accuracy: 1.4441e-04\n",
            "Epoch 22/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.0881 - accuracy: 1.4441e-04\n",
            "Epoch 23/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.0458 - accuracy: 1.4441e-04\n",
            "Epoch 24/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 29.0039 - accuracy: 1.4441e-04\n",
            "Epoch 25/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.9625 - accuracy: 1.4441e-04\n",
            "Epoch 26/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.9212 - accuracy: 1.4441e-04\n",
            "Epoch 27/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.8802 - accuracy: 1.4441e-04\n",
            "Epoch 28/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.8395 - accuracy: 1.4441e-04\n",
            "Epoch 29/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.7992 - accuracy: 1.4441e-04\n",
            "Epoch 30/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.7592 - accuracy: 1.4441e-04\n",
            "Epoch 31/100\n",
            "13849/13849 [==============================] - 0s 2us/step - loss: 28.7198 - accuracy: 1.4441e-04\n",
            "Epoch 32/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.6807 - accuracy: 1.4441e-04\n",
            "Epoch 33/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.6420 - accuracy: 1.4441e-04\n",
            "Epoch 34/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.6033 - accuracy: 1.4441e-04\n",
            "Epoch 35/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.5651 - accuracy: 1.4441e-04\n",
            "Epoch 36/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.5272 - accuracy: 1.4441e-04\n",
            "Epoch 37/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.4895 - accuracy: 1.4441e-04\n",
            "Epoch 38/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.4522 - accuracy: 1.4441e-04\n",
            "Epoch 39/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.4153 - accuracy: 1.4441e-04\n",
            "Epoch 40/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.3787 - accuracy: 1.4441e-04\n",
            "Epoch 41/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.3425 - accuracy: 1.4441e-04\n",
            "Epoch 42/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.3066 - accuracy: 1.4441e-04\n",
            "Epoch 43/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.2710 - accuracy: 1.4441e-04\n",
            "Epoch 44/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.2358 - accuracy: 1.4441e-04\n",
            "Epoch 45/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.2007 - accuracy: 1.4441e-04\n",
            "Epoch 46/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.1660 - accuracy: 1.4441e-04\n",
            "Epoch 47/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.1315 - accuracy: 1.4441e-04\n",
            "Epoch 48/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.0974 - accuracy: 1.4441e-04\n",
            "Epoch 49/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.0637 - accuracy: 1.4441e-04\n",
            "Epoch 50/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 28.0303 - accuracy: 1.4441e-04\n",
            "Epoch 51/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.9972 - accuracy: 1.4441e-04\n",
            "Epoch 52/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.9644 - accuracy: 1.4441e-04\n",
            "Epoch 53/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.9320 - accuracy: 1.4441e-04\n",
            "Epoch 54/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.8998 - accuracy: 1.4441e-04\n",
            "Epoch 55/100\n",
            "13849/13849 [==============================] - 0s 2us/step - loss: 27.8679 - accuracy: 1.4441e-04\n",
            "Epoch 56/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.8365 - accuracy: 1.4441e-04\n",
            "Epoch 57/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.8052 - accuracy: 1.4441e-04\n",
            "Epoch 58/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.7746 - accuracy: 1.4441e-04\n",
            "Epoch 59/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.7441 - accuracy: 1.4441e-04\n",
            "Epoch 60/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.7140 - accuracy: 1.4441e-04\n",
            "Epoch 61/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.6843 - accuracy: 1.4441e-04\n",
            "Epoch 62/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.6549 - accuracy: 1.4441e-04\n",
            "Epoch 63/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.6257 - accuracy: 1.4441e-04\n",
            "Epoch 64/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.5970 - accuracy: 1.4441e-04\n",
            "Epoch 65/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.5686 - accuracy: 1.4441e-04\n",
            "Epoch 66/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.5405 - accuracy: 1.4441e-04\n",
            "Epoch 67/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.5128 - accuracy: 1.4441e-04\n",
            "Epoch 68/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.4854 - accuracy: 1.4441e-04\n",
            "Epoch 69/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.4584 - accuracy: 1.4441e-04\n",
            "Epoch 70/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.4318 - accuracy: 1.4441e-04\n",
            "Epoch 71/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.4054 - accuracy: 1.4441e-04\n",
            "Epoch 72/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.3794 - accuracy: 1.4441e-04\n",
            "Epoch 73/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.3539 - accuracy: 1.4441e-04\n",
            "Epoch 74/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.3286 - accuracy: 1.4441e-04\n",
            "Epoch 75/100\n",
            "13849/13849 [==============================] - 0s 2us/step - loss: 27.3036 - accuracy: 1.4441e-04\n",
            "Epoch 76/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.2791 - accuracy: 1.4441e-04\n",
            "Epoch 77/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.2549 - accuracy: 1.4441e-04\n",
            "Epoch 78/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.2310 - accuracy: 1.4441e-04\n",
            "Epoch 79/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.2075 - accuracy: 1.4441e-04\n",
            "Epoch 80/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.1844 - accuracy: 1.4441e-04\n",
            "Epoch 81/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.1616 - accuracy: 1.4441e-04\n",
            "Epoch 82/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.1391 - accuracy: 1.4441e-04\n",
            "Epoch 83/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.1170 - accuracy: 1.4441e-04\n",
            "Epoch 84/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.0953 - accuracy: 1.4441e-04\n",
            "Epoch 85/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.0738 - accuracy: 1.4441e-04\n",
            "Epoch 86/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.0528 - accuracy: 1.4441e-04\n",
            "Epoch 87/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.0320 - accuracy: 1.4441e-04\n",
            "Epoch 88/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 27.0117 - accuracy: 1.4441e-04\n",
            "Epoch 89/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.9916 - accuracy: 1.4441e-04\n",
            "Epoch 90/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.9718 - accuracy: 1.4441e-04\n",
            "Epoch 91/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.9524 - accuracy: 1.4441e-04\n",
            "Epoch 92/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.9333 - accuracy: 1.4441e-04\n",
            "Epoch 93/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.9145 - accuracy: 1.4441e-04\n",
            "Epoch 94/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.8960 - accuracy: 1.4441e-04\n",
            "Epoch 95/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.8779 - accuracy: 1.4441e-04\n",
            "Epoch 96/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.8600 - accuracy: 1.4441e-04\n",
            "Epoch 97/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.8424 - accuracy: 1.4441e-04\n",
            "Epoch 98/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.8251 - accuracy: 1.4441e-04\n",
            "Epoch 99/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.8081 - accuracy: 1.4441e-04\n",
            "Epoch 100/100\n",
            "13849/13849 [==============================] - 0s 1us/step - loss: 26.7914 - accuracy: 1.4441e-04\n",
            "Accuracy: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ToaN_99Yf1o8"
      },
      "source": [
        "# Data Set 4 - Malnutrition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a80c5ca8-6d4a-489c-cdaa-1abd546cad8e",
        "id": "6oSnyceDf1pA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#LOAD DATA\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 500)\n",
        "data = pd.read_csv(\"/content/country-wise-average.csv\")\n",
        "\n",
        "from numpy import nan\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "#SPLIT INTO X AND Y\n",
        "X = data.drop(columns=[\"Income Classification\", \"Country\"]).astype('int32').to_numpy()\n",
        "y = data[\"Income Classification\"].to_numpy()\n",
        "\n",
        "#NORMALIZE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "#TRAINING AND TESTING SET\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .1)\n",
        "\n",
        "#### BUILDING THE MODEL ###\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model = Sequential()    # Specifies we are creating model sequentially \n",
        "                        #   and the each output layer is input to the next layer.\n",
        "\n",
        "# ADD EACH LAYER\n",
        "model.add(Dense(6, input_dim=6, activation='relu'))    # 'model.add' add a layer to our neural network.\n",
        "                                                        #   'Dense' specify fully connected layer.\n",
        "                                                        #       Arguments: Output(16), Input(20), Activation Function(relu)\n",
        "model.add(Dense(1, activation='sigmoid')) # X layer we dont need to specify input(16), the model to be sequential\n",
        "\n",
        "\n",
        "# LOSS FUNCTION/OPTIMIZER\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])   # 'Categorical_crossentropy' multiple classes.\n",
        "                                                                                    # Metrics is used to specify \n",
        "                                                                                    #   the way to judge the performance.\n",
        "                                                                                    # 'loss' what to use to evaluate a set of weights\n",
        "                                                                                    # 'optimizer' is used to search through different weights\n",
        "                                                                                    # “adam“ stochastic gradient descent algorithm \n",
        "                                                                                    # 'accuracy' because it is a classification problem\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#TRAINING\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=10000)    # Specify: \n",
        "                                                                    #   input data-> X_train, \n",
        "                                                                    #   labels-> y_train, \n",
        "                                                                    #   number of epochs(iterations), \n",
        "                                                                    #   and batch size. \n",
        "                                                                    # Returns history of model training.\n",
        "                                                                    # Batch Size: \n",
        "                                                                    #   Divides data into batches = batch_size. \n",
        "                                                                    #   Only this number of samples will be loaded/processed. \n",
        "                                                                    #   Once we are done with one batch, next batch will be processed.\n",
        "                                                                    # Epoch: One pass through all of the rows in the training dataset.\n",
        "                                                                    #   One epoch is comprised of one or more batches\n",
        "\n",
        "\n",
        "#Use your network to make predictions about a test set and note the success of the algorithm on your dataset.\n",
        "\n",
        "#Experiment with at least 3 different sets of hyper-parameters for the algorithm \n",
        "#   (e.g., different number of layers and nodes per layer, different learning rates, different activation functions, etc.).\n",
        "\n",
        "# check the model’s performance\n",
        "_, accuracy = model.evaluate(X, y, verbose=0)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "126/126 [==============================] - 0s 439us/step - loss: 1.8343 - accuracy: 0.1667\n",
            "Epoch 2/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.8320 - accuracy: 0.1667\n",
            "Epoch 3/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.8293 - accuracy: 0.1667\n",
            "Epoch 4/100\n",
            "126/126 [==============================] - 0s 23us/step - loss: 1.8264 - accuracy: 0.1667\n",
            "Epoch 5/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.8235 - accuracy: 0.1746\n",
            "Epoch 6/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.8205 - accuracy: 0.1746\n",
            "Epoch 7/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.8175 - accuracy: 0.1746\n",
            "Epoch 8/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.8144 - accuracy: 0.1746\n",
            "Epoch 9/100\n",
            "126/126 [==============================] - 0s 20us/step - loss: 1.8114 - accuracy: 0.1746\n",
            "Epoch 10/100\n",
            "126/126 [==============================] - 0s 12us/step - loss: 1.8084 - accuracy: 0.1746\n",
            "Epoch 11/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.8053 - accuracy: 0.1746\n",
            "Epoch 12/100\n",
            "126/126 [==============================] - 0s 12us/step - loss: 1.8023 - accuracy: 0.1825\n",
            "Epoch 13/100\n",
            "126/126 [==============================] - 0s 13us/step - loss: 1.7993 - accuracy: 0.1825\n",
            "Epoch 14/100\n",
            "126/126 [==============================] - 0s 13us/step - loss: 1.7963 - accuracy: 0.1746\n",
            "Epoch 15/100\n",
            "126/126 [==============================] - 0s 19us/step - loss: 1.7932 - accuracy: 0.1746\n",
            "Epoch 16/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.7902 - accuracy: 0.1825\n",
            "Epoch 17/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.7872 - accuracy: 0.1825\n",
            "Epoch 18/100\n",
            "126/126 [==============================] - 0s 22us/step - loss: 1.7841 - accuracy: 0.1825\n",
            "Epoch 19/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.7811 - accuracy: 0.1825\n",
            "Epoch 20/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.7782 - accuracy: 0.1825\n",
            "Epoch 21/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.7752 - accuracy: 0.1825\n",
            "Epoch 22/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.7723 - accuracy: 0.1825\n",
            "Epoch 23/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.7694 - accuracy: 0.1825\n",
            "Epoch 24/100\n",
            "126/126 [==============================] - 0s 13us/step - loss: 1.7666 - accuracy: 0.1825\n",
            "Epoch 25/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.7637 - accuracy: 0.1825\n",
            "Epoch 26/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.7608 - accuracy: 0.1825\n",
            "Epoch 27/100\n",
            "126/126 [==============================] - 0s 22us/step - loss: 1.7580 - accuracy: 0.1825\n",
            "Epoch 28/100\n",
            "126/126 [==============================] - 0s 13us/step - loss: 1.7552 - accuracy: 0.1905\n",
            "Epoch 29/100\n",
            "126/126 [==============================] - 0s 13us/step - loss: 1.7523 - accuracy: 0.1984\n",
            "Epoch 30/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.7495 - accuracy: 0.1984\n",
            "Epoch 31/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.7467 - accuracy: 0.1984\n",
            "Epoch 32/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.7439 - accuracy: 0.1984\n",
            "Epoch 33/100\n",
            "126/126 [==============================] - 0s 13us/step - loss: 1.7411 - accuracy: 0.1984\n",
            "Epoch 34/100\n",
            "126/126 [==============================] - 0s 19us/step - loss: 1.7383 - accuracy: 0.2063\n",
            "Epoch 35/100\n",
            "126/126 [==============================] - 0s 13us/step - loss: 1.7355 - accuracy: 0.2063\n",
            "Epoch 36/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.7328 - accuracy: 0.2063\n",
            "Epoch 37/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.7300 - accuracy: 0.2063\n",
            "Epoch 38/100\n",
            "126/126 [==============================] - 0s 18us/step - loss: 1.7272 - accuracy: 0.2063\n",
            "Epoch 39/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.7244 - accuracy: 0.2063\n",
            "Epoch 40/100\n",
            "126/126 [==============================] - 0s 13us/step - loss: 1.7215 - accuracy: 0.2143\n",
            "Epoch 41/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.7187 - accuracy: 0.2222\n",
            "Epoch 42/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.7158 - accuracy: 0.2302\n",
            "Epoch 43/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.7129 - accuracy: 0.2302\n",
            "Epoch 44/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.7100 - accuracy: 0.2302\n",
            "Epoch 45/100\n",
            "126/126 [==============================] - 0s 18us/step - loss: 1.7071 - accuracy: 0.2302\n",
            "Epoch 46/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.7042 - accuracy: 0.2302\n",
            "Epoch 47/100\n",
            "126/126 [==============================] - 0s 12us/step - loss: 1.7012 - accuracy: 0.2302\n",
            "Epoch 48/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.6982 - accuracy: 0.2302\n",
            "Epoch 49/100\n",
            "126/126 [==============================] - 0s 11us/step - loss: 1.6952 - accuracy: 0.2381\n",
            "Epoch 50/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.6922 - accuracy: 0.2460\n",
            "Epoch 51/100\n",
            "126/126 [==============================] - 0s 13us/step - loss: 1.6892 - accuracy: 0.2540\n",
            "Epoch 52/100\n",
            "126/126 [==============================] - 0s 12us/step - loss: 1.6862 - accuracy: 0.2540\n",
            "Epoch 53/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.6832 - accuracy: 0.2540\n",
            "Epoch 54/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.6802 - accuracy: 0.2619\n",
            "Epoch 55/100\n",
            "126/126 [==============================] - 0s 21us/step - loss: 1.6773 - accuracy: 0.2619\n",
            "Epoch 56/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.6743 - accuracy: 0.2619\n",
            "Epoch 57/100\n",
            "126/126 [==============================] - 0s 19us/step - loss: 1.6713 - accuracy: 0.2619\n",
            "Epoch 58/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.6683 - accuracy: 0.2619\n",
            "Epoch 59/100\n",
            "126/126 [==============================] - 0s 13us/step - loss: 1.6653 - accuracy: 0.2619\n",
            "Epoch 60/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.6623 - accuracy: 0.2619\n",
            "Epoch 61/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.6594 - accuracy: 0.2619\n",
            "Epoch 62/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.6564 - accuracy: 0.2619\n",
            "Epoch 63/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.6534 - accuracy: 0.2619\n",
            "Epoch 64/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.6505 - accuracy: 0.2619\n",
            "Epoch 65/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.6475 - accuracy: 0.2619\n",
            "Epoch 66/100\n",
            "126/126 [==============================] - 0s 22us/step - loss: 1.6445 - accuracy: 0.2619\n",
            "Epoch 67/100\n",
            "126/126 [==============================] - 0s 20us/step - loss: 1.6416 - accuracy: 0.2619\n",
            "Epoch 68/100\n",
            "126/126 [==============================] - 0s 22us/step - loss: 1.6387 - accuracy: 0.2619\n",
            "Epoch 69/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.6357 - accuracy: 0.2619\n",
            "Epoch 70/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.6328 - accuracy: 0.2619\n",
            "Epoch 71/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.6299 - accuracy: 0.2619\n",
            "Epoch 72/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.6269 - accuracy: 0.2698\n",
            "Epoch 73/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.6239 - accuracy: 0.2698\n",
            "Epoch 74/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.6209 - accuracy: 0.2698\n",
            "Epoch 75/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.6179 - accuracy: 0.2698\n",
            "Epoch 76/100\n",
            "126/126 [==============================] - 0s 21us/step - loss: 1.6150 - accuracy: 0.2698\n",
            "Epoch 77/100\n",
            "126/126 [==============================] - 0s 18us/step - loss: 1.6120 - accuracy: 0.2698\n",
            "Epoch 78/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.6089 - accuracy: 0.2698\n",
            "Epoch 79/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.6059 - accuracy: 0.2698\n",
            "Epoch 80/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.6027 - accuracy: 0.2698\n",
            "Epoch 81/100\n",
            "126/126 [==============================] - 0s 21us/step - loss: 1.5995 - accuracy: 0.2698\n",
            "Epoch 82/100\n",
            "126/126 [==============================] - 0s 19us/step - loss: 1.5963 - accuracy: 0.2698\n",
            "Epoch 83/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.5931 - accuracy: 0.2698\n",
            "Epoch 84/100\n",
            "126/126 [==============================] - 0s 14us/step - loss: 1.5899 - accuracy: 0.2698\n",
            "Epoch 85/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.5866 - accuracy: 0.2698\n",
            "Epoch 86/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.5833 - accuracy: 0.2698\n",
            "Epoch 87/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.5799 - accuracy: 0.2698\n",
            "Epoch 88/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.5765 - accuracy: 0.2698\n",
            "Epoch 89/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.5731 - accuracy: 0.2698\n",
            "Epoch 90/100\n",
            "126/126 [==============================] - 0s 23us/step - loss: 1.5697 - accuracy: 0.2698\n",
            "Epoch 91/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.5663 - accuracy: 0.2778\n",
            "Epoch 92/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.5630 - accuracy: 0.2778\n",
            "Epoch 93/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.5596 - accuracy: 0.2778\n",
            "Epoch 94/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.5561 - accuracy: 0.2778\n",
            "Epoch 95/100\n",
            "126/126 [==============================] - 0s 17us/step - loss: 1.5526 - accuracy: 0.2778\n",
            "Epoch 96/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.5491 - accuracy: 0.2778\n",
            "Epoch 97/100\n",
            "126/126 [==============================] - 0s 19us/step - loss: 1.5455 - accuracy: 0.2778\n",
            "Epoch 98/100\n",
            "126/126 [==============================] - 0s 19us/step - loss: 1.5420 - accuracy: 0.2778\n",
            "Epoch 99/100\n",
            "126/126 [==============================] - 0s 16us/step - loss: 1.5384 - accuracy: 0.2778\n",
            "Epoch 100/100\n",
            "126/126 [==============================] - 0s 15us/step - loss: 1.5349 - accuracy: 0.2778\n",
            "Accuracy: 30.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855-mmmKUfh1",
        "colab_type": "text"
      },
      "source": [
        "# Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHvWa9-MgfO2",
        "colab_type": "code",
        "outputId": "8b8f0596-9d86-4fc4-f859-174301791a00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "# Once the Tokenizer has been fit on training data \n",
        "#   it can be used to encode documents in the train or test datasets.\n",
        "# binary: Whether or not each word is present \n",
        "# count: Count of each word.\n",
        "# tfidf: scoring for each word.\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = ['Well done!',\n",
        "\t\t'Good work',\n",
        "\t\t'Great effort',\n",
        "\t\t'nice work',\n",
        "\t\t'Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)\n",
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n",
        "# integer encode documents\n",
        "b = t.texts_to_matrix(docs, mode='binary')\n",
        "c = t.texts_to_matrix(docs, mode='count')\n",
        "t = t.texts_to_matrix(docs, mode='tfidf')\n",
        "print(b)\n",
        "print()\n",
        "print(c)\n",
        "print()\n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
            "5\n",
            "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
            "defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'good': 1, 'work': 2, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n",
            "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            "[[0.         0.         1.25276297 1.25276297 0.         0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.98082925 0.         0.         1.25276297 0.\n",
            "  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         1.25276297\n",
            "  1.25276297 0.         0.        ]\n",
            " [0.         0.98082925 0.         0.         0.         0.\n",
            "  0.         1.25276297 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.25276297]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}